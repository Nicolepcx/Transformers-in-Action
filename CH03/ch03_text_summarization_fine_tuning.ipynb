{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Open notebook in: \n",
        "| Colab                                 | Kaggle                                        | Gradient                                                                                                                                         |\n",
        "|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nicolepcx/Transformers-in-Action/blob/main/CH03/ch03_text_summarization_fine_tuning.ipynb)                       | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com//Nicolepcx/Transformers-in-Action/blob/main/CH03/ch03_text_summarization_fine_tuning.ipynb)                       | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/Nicolepcx/Transformers-in-Action/blob/main/CH03/ch03_text_summarization_fine_tuning.ipynb)|             "
      ],
      "metadata": {
        "id": "5cO680Zol59t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download repo"
      ],
      "metadata": {
        "id": "1vnhHxCRlHRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment and run this cell if you're on Colab, Kaggle or Paperspace\n",
        "!git clone https://github.com/Nicolepcx/Transformers-in-Action.git\n",
        "\n",
        "current_path = %pwd\n",
        "if '/Transformers-in-Action' in current_path:\n",
        "    new_path = current_path + '/utils'\n",
        "else:\n",
        "    new_path = current_path + '/Transformers-in-Action/utils'\n",
        "%cd $new_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwX2FgaHT9Kd",
        "outputId": "484e498a-210e-4c70-cdd8-55e211ede303"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Transformers-in-Action' already exists and is not an empty directory.\n",
            "/content/Transformers-in-Action/utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About this notebook\n",
        "\n",
        "In this notebook, we will fine-tune the Facebook/BART-large-cnn model on the BillSum dataset. In the previous notebook, we explored and evaluated various text summarization models, including BART, and now we will take the next step by adapting the BART model specifically for summarizing legislative text data.\n",
        "\n",
        "Fine-tuning a pre-trained model can be a challenging process, as it requires a good understanding of the dataset, model architecture, and the underlying task. The process often involves hyperparameter optimization to find the best combination of settings that yield optimal performance. However, in this notebook, we will stick to the recommended model settings to simplify the fine-tuning process.\n",
        "\n",
        "In this notebook, you will:\n",
        "\n",
        "1. Gain an understanding of the fine-tuning process, its challenges, and the importance of hyperparameter optimization.\n",
        "2. Learn how to prepare the BillSum dataset for the fine-tuning process, including data preprocessing and splitting.\n",
        "3. Explore the recommended settings for the Facebook/BART-large-cnn model and understand their significance in the fine-tuning process.\n",
        "4. Implement the fine-tuning process using the recommended settings and observe how the model adapts to the legislative text data.\n",
        "5. Evaluate the fine-tuned model on multiple samples from the BillSum dataset, considering ROUGE as a comparison.\n",
        "6. Discuss the implications of fine-tuning with the recommended settings and the potential benefits of hyperparameter optimization for achieving better performance.\n",
        "\n",
        "By the end of this notebook, you will have a deeper understanding of the fine-tuning process, the challenges involved, and the importance of hyperparameter optimization. You will also gain practical experience in fine-tuning the Facebook/BART-large-cnn model on the BillSum dataset and evaluating its performance on legislative text data. Let's dive into the fascinating world of fine-tuning and enhance the capabilities of our text summarization model!"
      ],
      "metadata": {
        "id": "YMRPSK3ldaF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install requirements"
      ],
      "metadata": {
        "id": "bFkkn_xslQJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from requirements import *"
      ],
      "metadata": {
        "id": "CJQrCNtIxt7u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "install_base_packages()\n",
        "install_required_packages_ch03()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC7eYoG2nP4g",
        "outputId": "10f011c2-5da9-41b4-ff2f-1d6a8ebcd12b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInstalling base requirements...\n",
            "\u001b[0m\n",
            "âœ… transformers==4.26.1 installation completed successfully!\n",
            "\n",
            "âœ… datasets==2.10.1 installation completed successfully!\n",
            "\n",
            "\u001b[1mInstalling chapter 3 requirements...\n",
            "\u001b[0m\n",
            "âœ… summa==1.2.0 installation completed successfully!\n",
            "\n",
            "âœ… evaluate==0.4.0 installation completed successfully!\n",
            "\n",
            "âœ… rouge_score==0.1.2 installation completed successfully!\n",
            "\n",
            "âœ… sentencepiece installation completed successfully!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "LXhy8Pu_lVcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from setup import *\n",
        "from utils import *"
      ],
      "metadata": {
        "id": "73osbzDDpDkp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "useGPU()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVN6H8w-lcz8",
        "outputId": "a6026338-ce8c-49db-ba3a-16a4899a00ec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Have fun with this chapter!ðŸ¥³\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Allocate enough RAM"
      ],
      "metadata": {
        "id": "vazshSUmPs-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try to get a __GPU__ with at least __15GB RAM__ for our notebook."
      ],
      "metadata": {
        "id": "KEBmtiOeOOV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# crash colab to get more RAM -> uncomment to use\n",
        "#!kill -9 -1"
      ],
      "metadata": {
        "id": "dqsj2G_XOV5M"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can execute the following command `!free -h`  to see if we have enough RAM and `!nvidia-smi` to get more info about our GPU type we got assigned.\n",
        "If the allocated GPU is too small, the above cell can be used to run the command to crash the notebook hoping to get a better GPU after the crash, since the GPU is randomly allocated.\n"
      ],
      "metadata": {
        "id": "RMzVL7cLObmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!free -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrKOwtesglSu",
        "outputId": "68210b52-f4c9-4658-9cba-6456d123785d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:           83Gi       1.3Gi        80Gi       1.0Mi       1.9Gi        81Gi\n",
            "Swap:            0B          0B          0B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "LKRiHlhcOgMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f6faf7a-aeae-4884-95b9-cc1289fbc828"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar 16 11:32:50 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   29C    P0    52W / 400W |      3MiB / 40960MiB |     21%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_device = torch.device('cuda')\n",
        "    gpu_info = torch.cuda.get_device_properties(gpu_device)\n",
        "    gpu_memory = gpu_info.total_memory / 1e9  # Convert bytes to gigabytes\n",
        "    print(f\"GPU: {gpu_info.name}, Total Memory: {gpu_memory:.2f} GB\")\n",
        "else:\n",
        "    print(\"No GPU detected.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9cX1XaSsodd",
        "outputId": "408fc55a-c467-489b-ced8-d4c9432b2163"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: NVIDIA A100-SXM4-40GB, Total Memory: 42.48 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb=32\"\n"
      ],
      "metadata": {
        "id": "VMGKNqhQ8v5M"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments \n",
        "from transformers import EarlyStoppingCallback, DataCollatorForSeq2Seq, set_seed\n",
        "from evaluate import load as load_evaluate_metric\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "import numpy as np\n",
        "import warnings\n",
        "from transformers import logging\n",
        "from transformers import EvalPrediction\n",
        "from datasets import load_metric\n",
        "import evaluate\n",
        "\n"
      ],
      "metadata": {
        "id": "UELkXgo1zXc1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ],
      "metadata": {
        "id": "USBg-jsWM1Cb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the BillSum dataset\n",
        "dataset = load_dataset(\"billsum\")\n",
        "\n",
        "# Load BART tokenizer and model\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "8c0455153674465dbc6b8b4a91c19bc4",
            "ec77d54831a24d60967fc98a7efddf3a",
            "02013ca8474e4d5f9b8ce6d79bc73d52",
            "a9371142833f40b48515522f5f769004",
            "2db5836205fa4c4b9a52bb3ce4bde967",
            "2be6598ad67f4aafae2f1dc200f4772d",
            "9dff50f30d2d4cf7adcba70fd62c5c5d",
            "d6544e5b062341ab8a6ba49019576ecd",
            "b89f7ccfd96045b7b8a31e34552ed899",
            "196b5c9e8a534c6e96c972d16f5ee0d9",
            "8fbd55ea8ef445e39862b09c0e37a8e9"
          ]
        },
        "id": "jO2DlG4jOjK_",
        "outputId": "03fef5d9-8683-4ee9-c71c-ba88228c7a76"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset billsum (/root/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c0455153674465dbc6b8b4a91c19bc4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3GGnL-hRxk4",
        "outputId": "c67b4a0a-ecc4-441a-82ed-260b636f2c92"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'summary', 'title'],\n",
              "        num_rows: 18949\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'summary', 'title'],\n",
              "        num_rows: 3269\n",
              "    })\n",
              "    ca_test: Dataset({\n",
              "        features: ['text', 'summary', 'title'],\n",
              "        num_rows: 1237\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing and Subset Selection\n",
        "\n",
        "In this code snippet, we define a preprocessing function and apply it to a subset of the BillSum dataset for both training and testing purposes. The primary objective of this code is to prepare the data for the fine-tuning process of the Facebook/BART-large-cnn model, which we will use for legislative text summarization.\n",
        "\n",
        "1. `preprocess_function`: This function takes an example from the dataset, tokenizes the input text and the corresponding summary, and returns the input and target encodings in the required format. The input text is truncated or padded to a maximum length of 1024 tokens, while the summary is limited to 128 tokens.\n",
        "\n",
        "2. `set_range`: We set a range of 420 to select a subset of the dataset for training and testing purposes. This allows us to reduce the computational resources needed for the fine-tuning process while still providing a representative sample of the dataset.\n",
        "\n",
        "3. `subset_train` and `subset_test`: We select the specified range of examples from the \"train\" and \"ca_test\" splits of the dataset and apply the preprocess_function to each example using the `map()` function. This results in preprocessed training and testing subsets.\n",
        "\n",
        "4. `set_format`: We set the format of both subset_train and subset_test to \"torch\", which allows us to work with PyTorch tensors directly. We also specify the columns to include in the format: \"input_ids\", \"attention_mask\", and \"labels\".\n",
        "\n",
        "By applying this code, we efficiently preprocess and prepare the BillSum dataset for the fine-tuning process, ensuring that the data is compatible with the Facebook/BART-large-cnn model and facilitating an effective training and evaluation workflow."
      ],
      "metadata": {
        "id": "H_6_EtDUe31b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(example):\n",
        "    input_encoding = tokenizer(example[\"text\"], max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    target_encoding = tokenizer(example[\"summary\"], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    return {\"input_ids\": input_encoding[\"input_ids\"][0], \"attention_mask\": input_encoding[\"attention_mask\"][0], \"labels\": target_encoding[\"input_ids\"][0]}\n",
        "\n",
        "\n",
        "subset_train = dataset[\"train\"].select(range(800)).map(preprocess_function)\n",
        "subset_test = dataset[\"ca_test\"].select(range(400)).map(preprocess_function)\n",
        "\n",
        "subset_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "subset_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54,
          "referenced_widgets": [
            "f7e1a2c36f9342629541da0230106a74",
            "d7027855e8124948899abe75cf95694a",
            "c456742920f748cf829bbd65ff0def11",
            "e07c541eb52b434ab99b9a55c9dc5f6e",
            "28ee590379b24ebb990177700c1d9530",
            "4c879394f29f4a17908b04da46569516",
            "94f0d85019484ee490f4f94c3613808f",
            "e3c1e93de47c4f8cbe1206dc410eedfe",
            "edb8910f1b4a4343934d6dce5bcd3117",
            "f97ca81966454f9c98dc56940500d745",
            "30e51c79ea674b79850ac8ae6c5b8b5c"
          ]
        },
        "id": "agzXruugR30P",
        "outputId": "551a4377-559e-42c7-c8ca-1b509fef197e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc/cache-cf97bdbf9a7f1f13.arrow\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7e1a2c36f9342629541da0230106a74"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZtcd1Y9rbzk",
        "outputId": "ab3e68ba-34dc-4785-aa2f-f7b74f1b5c4c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'summary', 'title', 'input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 800\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Computing ROUGE Scores for Model Evaluation\n",
        "\n",
        "\n",
        "This code snippet defines a function called compute_rouge, which calculates the ROUGE score for the model's predictions during evaluation. The function takes an EvalPrediction object as input, which contains the model's generated predictions and the corresponding reference summaries (ground truth).\n",
        "\n",
        "Here's a step-by-step explanation of the function:\n",
        "\n",
        "1. `rouge = evaluate.load('rouge')`: We load the ROUGE metric from the Hugging Face datasets library using the load() function. This metric will be used to calculate the similarity between the model's predictions and the reference summaries.\n",
        "\n",
        "2. `predictions = tokenizer.batch_decode(eval_prediction.predictions, skip_special_tokens=True)`: We decode the model's predictions stored in `eval_prediction`.predictions using the tokenizer's `batch_decode()` function. We set skip_special_tokens=True to remove any special tokens (e.g., padding or EOS tokens) from the decoded text.\n",
        "\n",
        "3. `references = tokenizer.batch_decode(eval_prediction.label_ids, skip_special_tokens=True)`: Similarly, we decode the reference summaries (ground truth) stored in `eval_prediction.label_ids` using the tokenizer's `batch_decode()` function, while also removing any special tokens.\n",
        "\n",
        "4. `return rouge.compute(predictions=predictions, references=references)`: We compute the ROUGE score using the `rouge.compute()` function with the decoded predictions and reference summaries as input. The function returns the calculated ROUGE scores, which serve as the evaluation metric for the model's performance.\n",
        "\n",
        "We need this function for the Trainer class provided by the Hugging Face transformers library. The Trainer class requires a custom function to compute metrics during the evaluation phase. By defining compute_rouge, we can integrate the ROUGE metric into the training and evaluation workflow, allowing us to assess the model's performance in generating summaries."
      ],
      "metadata": {
        "id": "aijExS3xfzlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rouge(eval_prediction: EvalPrediction):\n",
        "    rouge = evaluate.load('rouge')\n",
        "    predictions = tokenizer.batch_decode(eval_prediction.predictions, skip_special_tokens=True)\n",
        "    references = tokenizer.batch_decode(eval_prediction.label_ids, skip_special_tokens=True)\n",
        "\n",
        "    return rouge.compute(predictions=predictions, references=references)\n",
        "\n"
      ],
      "metadata": {
        "id": "8n6dEa0KYATn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Training Arguments and Initializing Data Collator\n",
        "\n",
        "\n",
        "This code snippet sets up the training arguments and initializes a data collator for fine-tuning the text summarization model. The TrainingArguments class from the Hugging Face transformers library is used to define various hyperparameters and settings for the training process.\n",
        "\n",
        "1. `output_dir`: Specifies the directory where the trained model and related files will be saved.\n",
        "2. `num_train_epochs`: Sets the number of training epochs to 3, which is within the recommended range.\n",
        "3. `learning_rate`: Sets the learning rate to 3e-5, which is the recommended value for this model.\n",
        "4. `per_device_train_batch_size` and `per_device_eval_batch_size`: Set the batch size for training and evaluation to 8, as recommended.\n",
        "5. `gradient_accumulation_steps`: Sets the number of steps to accumulate gradients before performing an optimizer step. The recommended value of 1 is used here.\n",
        "6. `warmup_steps`: Specifies the number of warm-up steps for the learning rate scheduler. The recommended value of 500 is used.\n",
        "7. `weight_decay`: Sets the weight decay for the optimizer to 0.01, as recommended.\n",
        "8. `logging_dir, logging_steps`: Define the directory for logging and the number of steps between each logging.\n",
        "9. `evaluation_strategy`, `eval_steps`: Set the evaluation strategy to 'steps' and the number of steps between each evaluation to 160.\n",
        "10. `load_best_model_at_end`: Indicates whether to load the best model found during training at the end of the process.\n",
        "11. `save_steps`: Specifies the number of steps between each checkpoint save.\n",
        "12. `seed`: Sets the random seed for reproducibility.\n",
        "13. `report_to`: Disables reporting to external platforms by setting it to 'none'.\n",
        "\n",
        "Next, the `DataCollatorForSeq2Seq` class is used to initialize a data collator. This collator is responsible for preparing batches of data during training. It takes the tokenizer, model, padding setting, and maximum length as input. The data collator ensures that the input sequences are padded or truncated to the specified maximum length (1024 tokens) and organizes the data into the required format for training the seq2seq model."
      ],
      "metadata": {
        "id": "Rcvrwocog1TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,                 # Keep this as it is within the recommended range\n",
        "    learning_rate=3e-5,                 # Use the recommended learning rate\n",
        "    per_device_train_batch_size=8,      # Use  the recommended batch size\n",
        "    per_device_eval_batch_size=8,       # Use the recommended batch size\n",
        "    gradient_accumulation_steps=1,      # Use the recommended -> 1 by default\n",
        "    warmup_steps=500,                   # Use the recommended warm-up steps\n",
        "    weight_decay=0.01,                  # Use the recommended value\n",
        "    logging_dir='logs',\n",
        "    logging_steps=160,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=160,\n",
        "    load_best_model_at_end=True,\n",
        "    save_steps=160,\n",
        "    seed=42,\n",
        "    report_to='none'\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize the data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True, max_length=1024)\n"
      ],
      "metadata": {
        "id": "QZK-gDrEPIOo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning the Model with Trainer and Early Stopping Callback\n",
        "\n",
        "This code snippet initializes and trains a Trainer object from the Hugging Face transformers library, which simplifies the fine-tuning process for our text summarization model. The Trainer class provides a high-level interface for training, fine-tuning, and evaluating models, streamlining the overall process and reducing boilerplate code.\n",
        "\n",
        "The Trainer object is initialized with the following parameters:\n",
        "\n",
        "1. `model`: The pre-trained model to be fine-tuned.\n",
        "2. `args`: The TrainingArguments object containing the hyperparameters and settings for the training process.\n",
        "3. `train_dataset` and `eval_dataset`: The preprocessed training and evaluation subsets of the BillSum dataset.\n",
        "4. `compute_metrics`: The custom metric function, compute_rouge, which calculates ROUGE scores during evaluation.\n",
        "5. `data_collator`: The DataCollatorForSeq2Seq object responsible for preparing batches of data during training.\n",
        "6. `callbacks`: A list of callbacks, including the EarlyStoppingCallback, which monitors training progress and stops training if there is no significant improvement in the evaluation metric within a specified number of evaluation steps.\n",
        "\n",
        "The benefits of using the Trainer class include:\n",
        "\n",
        "- Simplified training and evaluation loop, allowing for easy implementation and reduced development time.\n",
        "- Built-in support for distributed training and mixed precision, improving training efficiency.\n",
        "- Automatic handling of learning rate scheduling, optimization, and gradient accumulation.\n",
        "- Integration with custom metric functions, such as compute_rouge, for model evaluation.\n",
        "- The EarlyStoppingCallback is an optional feature that monitors the model's performance during training and stops the process early if there is no significant improvement within a specified number of evaluation steps. This callback helps prevent overfitting and can save time and computational resources by terminating training when the model's performance plateaus.\n",
        "\n",
        "In this code snippet, the EarlyStoppingCallback is set to monitor the evaluation metric with a patience of 10 evaluation steps and a minimum improvement threshold of 0.001. If the metric does not improve by at least 0.001 within 10 evaluation steps, the training process will be terminated.\n",
        "\n",
        "Finally, the trainer.train() function is called to start the fine-tuning process for the text summarization model using the specified parameters and settings."
      ],
      "metadata": {
        "id": "yLAtdbyopdWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=subset_train,\n",
        "    eval_dataset=subset_test,\n",
        "    compute_metrics=compute_rouge,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[EarlyStoppingCallback(10, 0.001)]\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "wIJkf-dPYDcu",
        "outputId": "1a2f7b5a-0d77-46a8-8134-561a5d52325b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, text, title. If summary, text, title are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 800\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 100\n",
            "  Number of trainable parameters = 406290432\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2357: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 01:25, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=2.9135833740234376, metrics={'train_runtime': 89.5806, 'train_samples_per_second': 8.931, 'train_steps_per_second': 1.116, 'total_flos': 1733683681689600.0, 'train_loss': 2.9135833740234376, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(dataset[\"test\"][\"text\"][0])\n",
        "type(dataset[\"ca_test\"][\"summary\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s6yM_skaxLD",
        "outputId": "cfef817b-1c59-42aa-f0e1-971b54a6cd5d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This line suppresses UserWarnings originating from the transformers module. \n",
        "# By setting the filter to ignore warnings, you can avoid having your notebook \n",
        "# cluttered with potentially unnecessary warning messages while running your code.\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers\")\n",
        "\n",
        "# This line sets the verbosity level of the logging messages to ERROR. By doing \n",
        "# this, only error messages will be displayed in the notebook, hiding other log \n",
        "# messages with lower severity levels such as DEBUG, INFO, and WARNING. \n",
        "# This helps to keep the notebook output focused on errors that may require \n",
        "# your attention.\n",
        "logging.set_verbosity(logging.ERROR)\n"
      ],
      "metadata": {
        "id": "XYoQWP3SAdk8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function for Generating Summaries with a Fine-Tuned Mode\n",
        "\n",
        "Info: This code defines a function called `generate_summary`, which takes a fine-tuned model, tokenizer, and an input text as arguments. The function generates a summary for the given input text using the model and tokenizer.\n",
        "\n",
        "1. The device variable is set to use a GPU if available, otherwise, it falls back to using the CPU.\n",
        "2. The model is moved to the appropriate device using model.to(device).\n",
        "3. The `max_length` variable is set to the tokenizer's model maximum length.\n",
        "4. The input text is split into chunks based on the `max_length` to avoid exceeding the model's token limit.\n",
        "5. An empty list called summaries is initialized to store the generated summaries for each chunk.\n",
        "6. The function iterates over each chunk, encoding it using the tokenizer and moving the encoding to the appropriate device.\n",
        "7. The `model.generate()` function is called with the input encoding, specifying parameters like `max_length`, `min_length`, length_penalty, and num_beams to control the generation process.\n",
        "8. The generated summary is decoded using the tokenizer and appended to the summaries list.\n",
        "9. The final summary is created by joining the summaries of each chunk using spaces and returned by the function.\n",
        "\n",
        "This function can be used to generate summaries for long input texts with the fine-tuned model and tokenizer."
      ],
      "metadata": {
        "id": "hr3KHUTf2tmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(model, tokenizer, input_text):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    max_length = tokenizer.model_max_length\n",
        "    chunks = [input_text[i:i + max_length] for i in range(0, len(input_text), max_length)]\n",
        "\n",
        "    summaries = []\n",
        "    for chunk in chunks:\n",
        "        # pt = return PyTorch tensors\n",
        "        input_encoding = tokenizer(chunk, max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
        "        input_encoding = input_encoding.to(device)\n",
        "\n",
        "        summary_ids = model.generate(input_encoding[\"input_ids\"], max_length=128, min_length=32, length_penalty=2.0, num_beams=4)\n",
        "        summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        summaries.append(summary_text)\n",
        "\n",
        "    summary = \" \".join(summaries)\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "VMw55DduDH-l"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Â Evaluating the Model on 10 Test Samples and Computing ROUGE Scores\n",
        "\n",
        "This following code evaluates the fine-tuned model on 10 samples from the test dataset and computes the ROUGE scores for the generated summaries.\n",
        "\n",
        "- `generated_summaries`: A list comprehension is used to call the generate_summary function for each of the first 10 samples in the ca_test dataset. This generates a list of summaries produced by the model for each input text.\n",
        "\n",
        "- `reference_summaries`: A list comprehension is used to extract the first 10 reference summaries from the ca_test dataset. These reference summaries serve as the ground truth for comparison with the generated summaries.\n",
        "\n",
        "- `rouge = evaluate.load('rouge')`: The ROUGE evaluation metric is loaded using the evaluate library.\n",
        "\n",
        "- `rouge_scores`: The rouge.compute() method is called with the predictions (generated_summaries) and references (reference_summaries) as its arguments. This computes the ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L, and ROUGELsum) for the generated summaries compared to the reference summaries.\n",
        "\n",
        "We evaluate the model on different variants of ROUGE, which consider different aspects of the summaries, such as n-grams and the longest common subsequence. ROUGE was introduced in the paper titled \"[ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013.pdf)\" by Chin-Yew Lin in 2004\n",
        "\n",
        "__ROUGE-1__: This metric measures the unigram (single-word) overlap between the generated summary and the reference summary. It is based on the recall of unigrams in the generated summary with respect to the reference summary. The formula for ROUGE-1 recall is:\n",
        "\n",
        "$$R_{1} = \\frac{\\sum_{i}^{n} \\text{min}(g_i, r_i)}{\\sum_{i}^{n} r_i}$$\n",
        "\n",
        "where $g_i$ and $r_i$ represent the count of the unigram $i$ in the generated and reference summaries, respectively, and $n$ is the number of unique unigrams.\n",
        "\n",
        "__ROUGE-2__: This metric measures the bigram (two-word sequence) overlap between the generated summary and the reference summary. It is based on the recall of bigrams in the generated summary with respect to the reference summary. The formula for ROUGE-2 recall is:\n",
        "\n",
        "$$R_{2} = \\frac{\\sum_{i}^{n} \\text{min}(g_i, r_i)}{\\sum_{i}^{n} r_i}$$\n",
        "\n",
        "where $g_i$ and $r_i$ represent the count of the bigram $i$ in the generated and reference summaries, respectively, and $n$ is the number of unique bigrams.\n",
        "\n",
        "__ROUGE-L__: This metric measures the longest common subsequence (LCS) between the generated summary and the reference summary. It is based on the recall of the longest common subsequence in the generated summary with respect to the reference summary. The formula for ROUGE-L recall is:\n",
        "\n",
        "$$R_{L} = \\frac{\\text{LCS}(g, r)}{\\text{len}(r)}$$\n",
        "\n",
        "where $\\text{LCS}(g, r)$ is the length of the longest common subsequence between the generated summary $g$ and the reference summary $r$, and $\\text{len}(r)$ is the length of the reference summary.\n",
        "\n",
        "__ROUGE-Lsum__: This metric is an extension of ROUGE-L that computes the longest common subsequence for each sentence in the generated summary and the reference summary, and then takes the sum of the longest common subsequences. It is designed to handle cases where the summaries have multiple sentences.\n",
        "\n",
        "__Note__: It's good practice to evaluate your model on different metrics and also compare it to a state-of-the-art model using the same dataset, if available. "
      ],
      "metadata": {
        "id": "16B6lnT66J72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on 10 samples\n",
        "generated_summaries = [generate_summary(model, tokenizer, dataset[\"ca_test\"][\"text\"][i]) for i in range(10)]\n",
        "reference_summaries = [dataset[\"ca_test\"][\"summary\"][i] for i in range(10)]\n",
        "\n",
        "rouge = evaluate.load('rouge')\n",
        "rouge_scores = rouge.compute(predictions=generated_summaries, references=reference_summaries)\n",
        "print(rouge_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRI2FD9yDMhc",
        "outputId": "1f12dff7-d493-4c2a-d925-d1582b76f7fd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': 0.4773556924232474, 'rouge2': 0.23361950862025824,\n",
            "'rougeL': 0.257680948816016, 'rougeLsum': 0.34846892620876274}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data\n",
        "pd.DataFrame.from_records(rouge_scores, index=[\"BART\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "a4Qmhm0VyIZ9",
        "outputId": "f430613f-8389-4914-e7b3-ef8e8ff615e7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        rouge1   rouge2    rougeL  rougeLsum\n",
              "BART  0.477356  0.23362  0.257681   0.348469"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f767b46-3a40-4851-ae69-3e31ba630958\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rouge1</th>\n",
              "      <th>rouge2</th>\n",
              "      <th>rougeL</th>\n",
              "      <th>rougeLsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>BART</th>\n",
              "      <td>0.477356</td>\n",
              "      <td>0.23362</td>\n",
              "      <td>0.257681</td>\n",
              "      <td>0.348469</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f767b46-3a40-4851-ae69-3e31ba630958')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0f767b46-3a40-4851-ae69-3e31ba630958 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0f767b46-3a40-4851-ae69-3e31ba630958');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " print(dataset[\"ca_test\"][\"text\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSYZuPIyxO73",
        "outputId": "bc4ec65c-043a-4dc6-8f44-36435892121c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The people of the State of California do enact as follows:\n",
            "\n",
            "\n",
            "SECTION 1.\n",
            "The Legislature finds and declares all of the following:\n",
            "(a) (1) Since 1899 congressionally chartered veteransâ€™ organizations\n",
            "have provided a valuable service to our nationâ€™s returning service\n",
            "members. These organizations help preserve the memories and incidents\n",
            "of the great hostilities fought by our nation, and preserve and\n",
            "strengthen comradeship among members.\n",
            "(2) These veteransâ€™ organizations also own and manage various\n",
            "properties including lodges, posts, and fraternal halls. These\n",
            "properties act as a safe haven where veterans of all ages and their\n",
            "families can gather together to find camaraderie and fellowship, share\n",
            "stories, and seek support from people who understand their unique\n",
            "experiences. This aids in the healing process for these returning\n",
            "veterans, and ensures their health and happiness.\n",
            "(b) As a result of congressional chartering of these veteransâ€™\n",
            "organizations, the United States Internal Revenue Service created a\n",
            "special tax exemption for these organizations under Section 501(c)(19)\n",
            "of the Internal Revenue Code.\n",
            "(c) Section 501(c)(19) of the Internal Revenue Code and related\n",
            "federal regulations provide for the exemption for posts or\n",
            "organizations of war veterans, or an auxiliary unit or society of, or\n",
            "a trust or foundation for, any such post or organization that, among\n",
            "other attributes, carries on programs to perpetuate the memory of\n",
            "deceased veterans and members of the Armed Forces and to comfort their\n",
            "survivors, conducts programs for religious, charitable, scientific,\n",
            "literary, or educational purposes, sponsors or participates in\n",
            "activities of a patriotic nature, and provides social and recreational\n",
            "activities for their members.\n",
            "(d) Section 215.1 of the Revenue and Taxation Code stipulates that all\n",
            "buildings, support and so much of the real property on which the\n",
            "buildings are situated as may be required for the convenient use and\n",
            "occupation of the buildings, used exclusively for charitable purposes,\n",
            "owned by a veteransâ€™ organization that has been chartered by the\n",
            "Congress of the United States, organized and operated for charitable\n",
            "purposes, when the same are used solely and exclusively for the\n",
            "purpose of the organization, if not conducted for profit and no part\n",
            "of the net earnings of which ensures to the benefit of any private\n",
            "individual or member thereof, are exempt from taxation.\n",
            "(e) The Chief Counsel of the State Board of Equalization concluded,\n",
            "based on a 1979 appellate court decision, that only parts of American\n",
            "Legion halls are exempt from property taxation and that other parts,\n",
            "such as billiard rooms, card rooms, and similar areas, are not exempt.\n",
            "(f) In a 1994 memorandum, the State Board of Equalizationâ€™s legal\n",
            "division further concluded that the areas normally considered eligible\n",
            "for exemptions are the office areas used to counsel veterans and the\n",
            "area used to store veteransâ€™ records, but that the meeting hall and\n",
            "bar found in most of the facilities are not considered used for\n",
            "charitable purposes.\n",
            "(g) Tax-exempt status is intended to provide economic incentive and\n",
            "support to veteransâ€™ organizations to provide for the social welfare\n",
            "of the community of current and former military personnel.\n",
            "(h) The State Board of Equalizationâ€™s constriction of the tax\n",
            "exemption has resulted in an onerous tax burden on California veteran\n",
            "service organizations posts or halls, hinders the postsâ€™ ability to\n",
            "provide facilities for veterans, and threatens the economic viability\n",
            "of many local organizations.\n",
            "(i) The charitable activities of a veteran service organizations post\n",
            "or hall are much more than the counseling of veterans. The\n",
            "requirements listed for qualification for the federal tax exemption\n",
            "clearly dictate a need for more than just an office.\n",
            "(j) Programs to perpetuate the memory of deceased veterans and members\n",
            "of the Armed Forces and to comfort their survivors require the use of\n",
            "facilities for funerals and receptions.\n",
            "(k) Programs for religious, charitable, scientific, literary, or\n",
            "educational purposes require space for more than 50 attendees.\n",
            "(l) Activities of a patriotic nature need facilities to accommodate\n",
            "hundreds of people.\n",
            "(m) Social and recreational activities for members require precisely\n",
            "those areas considered â€œnot used for charitable purposesâ€ by the State\n",
            "Board of Equalization.\n",
            "(n) The State Board of Equalizationâ€™s interpretation of the Revenue\n",
            "and Taxation Code reflects a lack of understanding of the purpose and\n",
            "programs of the veterans service organizations posts or halls and is\n",
            "detrimental to the good works performed in support of our veteran\n",
            "community.\n",
            "SECTION 1.\n",
            "SEC. 2.\n",
            "Section 215.1 of the Revenue and Taxation Code is amended to read:\n",
            "215.1.\n",
            "(a) All buildings, and so much of the real property on which the\n",
            "buildings are situated as may be required for the convenient use and\n",
            "occupation of the buildings, used exclusively for charitable purposes,\n",
            "owned by a veteransâ€™ organization that has been chartered by the\n",
            "Congress of the United States, organized and operated for charitable\n",
            "purposes, and exempt from federal income tax as an organization\n",
            "described in Section 501(c)(19) of the Internal Revenue Code when the\n",
            "same are used solely and exclusively for the purpose of the\n",
            "organization, if not conducted for profit and no part of the net\n",
            "earnings of which inures to the benefit of any private individual or\n",
            "member thereof, shall be exempt from taxation.\n",
            "(b) The exemption provided for in this section shall apply to the\n",
            "property of all organizations meeting the requirements of this\n",
            "section, subdivision (b) of Section 4 of Article XIII of the\n",
            "California Constitution, and paragraphs (1) to (4), inclusive, (6),\n",
            "and (7) of subdivision (a) of Section 214.\n",
            "(c) (1) The exemption specified by subdivision (a) shall not be denied\n",
            "to a property on the basis that the property is used for fraternal,\n",
            "lodge, or social club purposes.\n",
            "(2) With regard to this subdivision, the Legislature finds and\n",
            "declares all of the following:\n",
            "(A) The exempt activities of a veteransâ€™ organization as described in\n",
            "subdivision (a) qualitatively differ from the exempt activities of\n",
            "other nonprofit entities that use property for fraternal, lodge, or\n",
            "social club purposes in that the exempt purpose of the veteransâ€™\n",
            "organization is to conduct programs to perpetuate the memory of\n",
            "deceased veterans and members of the Armed Forces and to comfort their\n",
            "survivors, to conduct programs for religious, charitable, scientific,\n",
            "literary, or educational purposes, to sponsor or participate in\n",
            "activities of a patriotic nature, and to provide social and\n",
            "recreational activities for their members.\n",
            "(B) In light of this distinction, the use of real property by a\n",
            "veteransâ€™ organization as described in subdivision (a), for fraternal,\n",
            "lodge, or social club purposes is central to that organizationâ€™s\n",
            "exempt purposes and activities.\n",
            "(C) In light of the factors set forth in subparagraphs (A) and (B),\n",
            "the use of real property by a veteransâ€™ organization as described in\n",
            "subdivision (a) for fraternal, lodge, or social club purposes,\n",
            "constitutes the exclusive use of that property for a charitable\n",
            "purpose within the meaning of subdivision (b) of Section 4 of Article\n",
            "XIII of the California Constitution.\n",
            "(d) The exemption provided for in this section shall not apply to any\n",
            "portion of a property that consists of a bar where alcoholic beverages\n",
            "are served. The portion of the property ineligible for the veteransâ€™\n",
            "organization exemption shall be that area used primarily to prepare\n",
            "and serve alcoholic beverages.\n",
            "(e) An organization that files a claim for the exemption provided for\n",
            "in this section shall file with the assessor a valid organizational\n",
            "clearance certificate issued pursuant to Section 254.6.\n",
            "(f) This exemption shall be known as the â€œveteransâ€™ organization\n",
            "exemption.â€\n",
            "SEC. 2.\n",
            "SEC. 3.\n",
            "Notwithstanding Section 2229 of the Revenue and Taxation Code, no\n",
            "appropriation is made by this act and the state shall not reimburse\n",
            "any local agency for any property tax revenues lost by it pursuant to\n",
            "this act.\n",
            "SEC. 3.\n",
            "SEC. 4.\n",
            "This act provides for a tax levy within the meaning of Article IV of\n",
            "the Constitution and shall go into immediate effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_summaries[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fdvE2ti9l8k",
        "outputId": "32b9cea6-3976-4e96-e30b-39dd35549989"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Since 1899, congressionally chartered veterans' organizations have\n",
            "provided a valuable service to our nation's returning service members.\n",
            "These organizations help preserve the memories and incidents of the\n",
            "great hostilities fought by our nation, and preserve and strengthen\n",
            "comradeship among members. The Internal Revenue Code provides for the\n",
            "exemption for organizations of war veterans, or an auxiliary unit or\n",
            "society of, or a trust or foundation for, any such post or\n",
            "organization that, among other attributes, carries on programs to\n",
            "perpetuate the memory of deceased veterans and members of the Armed\n",
            "Forces and to comfort their survivors. American Legion organizations\n",
            "chartered and operated for charitable purposes, when the same are used\n",
            "solely and exclusively for the purpose of the organization, if not\n",
            "conducted for profit and no part of the net earnings of which ensures\n",
            "to the benefit of any private individual or member thereof, are exempt\n",
            "from taxation. State Board of Equalization's constriction of the tax\n",
            "exemption has resulted in an onerous tax burden on California veteran\n",
            "service organizations posts or halls, hinders the postsâ€™ ability to\n",
            "provide facilities for veterans, and threatens the economic viability\n",
            "of many local organizations. State Board of Equalization's\n",
            "interpretation of the Revenue and Taxation Code reflects a lack of\n",
            "understanding of the purpose and programs of the veterans service\n",
            "organizations posts or halls and is detrimental to the good works\n",
            "performed in support of our veteran community. The exemption provided\n",
            "for in this section shall apply to the property of all organizations\n",
            "meeting the requirements of this section, subdivision (b) of Article\n",
            "XIII of the California Constitution, and paragraphs (1) to (4),\n",
            "inclusive, (6) and (7) of subdivision (a) of Section 214 of the\n",
            "Internal Revenue Code. The exemption shall not be denied to a property\n",
            "on the basis that the property is used for fraternal, lodge, or social\n",
            "club purposes. Use of real property by a veteransâ€™ organization as\n",
            "described in subdivision (a), for fraternal, lodge, or social club\n",
            "purposes is central to that organization's exempt purposes and\n",
            "activities. meaning of subdivision (b) of Section 4 of Article XIII of\n",
            "the California Constitution. The exemption provided for in this\n",
            "section shall not apply to any portion of a property that consists of\n",
            "a bar where alcoholic beverages are served. The portion of the\n",
            "property ineligible for the veterans' organization exemption shall be\n",
            "that area used to prepare and serve alcoholic beverages. ate effect.\n",
            "The effect of the increase in the number of women in the U.S. has been\n",
            "shown to have a positive effect on women's health.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reference_summaries[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajuHmwp8s9dd",
        "outputId": "7a554b8f-1202-4381-ea9d-614d4a8afb9c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing property tax law establishes a veteransâ€™ organization\n",
            "exemption under which property is exempt from taxation if, among other\n",
            "things, that property is used exclusively for charitable purposes and\n",
            "is owned by a veteransâ€™ organization.\n",
            "This bill would provide that the veteransâ€™ organization exemption\n",
            "shall not be denied to a property on the basis that the property is\n",
            "used for fraternal, lodge, or social club purposes, and would make\n",
            "specific findings and declarations in that regard. The bill would also\n",
            "provide that the exemption shall not apply to any portion of a\n",
            "property that consists of a bar where alcoholic beverages are served.\n",
            "Section 2229 of the Revenue and Taxation Code requires the Legislature\n",
            "to reimburse local agencies annually for certain property tax revenues\n",
            "lost as a result of any exemption or classification of property for\n",
            "purposes of ad valorem property taxation.\n",
            "This bill would provide that, notwithstanding Section 2229 of the\n",
            "Revenue and Taxation Code, no appropriation is made and the state\n",
            "shall not reimburse local agencies for property tax revenues lost by\n",
            "them pursuant to the bill.\n",
            "This bill would take effect immediately as a tax levy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yRm98pgYtTxU"
      },
      "execution_count": 27,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c0455153674465dbc6b8b4a91c19bc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec77d54831a24d60967fc98a7efddf3a",
              "IPY_MODEL_02013ca8474e4d5f9b8ce6d79bc73d52",
              "IPY_MODEL_a9371142833f40b48515522f5f769004"
            ],
            "layout": "IPY_MODEL_2db5836205fa4c4b9a52bb3ce4bde967"
          }
        },
        "ec77d54831a24d60967fc98a7efddf3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2be6598ad67f4aafae2f1dc200f4772d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9dff50f30d2d4cf7adcba70fd62c5c5d",
            "value": "100%"
          }
        },
        "02013ca8474e4d5f9b8ce6d79bc73d52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6544e5b062341ab8a6ba49019576ecd",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b89f7ccfd96045b7b8a31e34552ed899",
            "value": 3
          }
        },
        "a9371142833f40b48515522f5f769004": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_196b5c9e8a534c6e96c972d16f5ee0d9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8fbd55ea8ef445e39862b09c0e37a8e9",
            "value": " 3/3 [00:00&lt;00:00,  5.35it/s]"
          }
        },
        "2db5836205fa4c4b9a52bb3ce4bde967": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2be6598ad67f4aafae2f1dc200f4772d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dff50f30d2d4cf7adcba70fd62c5c5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6544e5b062341ab8a6ba49019576ecd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b89f7ccfd96045b7b8a31e34552ed899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "196b5c9e8a534c6e96c972d16f5ee0d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fbd55ea8ef445e39862b09c0e37a8e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7e1a2c36f9342629541da0230106a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7027855e8124948899abe75cf95694a",
              "IPY_MODEL_c456742920f748cf829bbd65ff0def11",
              "IPY_MODEL_e07c541eb52b434ab99b9a55c9dc5f6e"
            ],
            "layout": "IPY_MODEL_28ee590379b24ebb990177700c1d9530"
          }
        },
        "d7027855e8124948899abe75cf95694a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c879394f29f4a17908b04da46569516",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_94f0d85019484ee490f4f94c3613808f",
            "value": "Map:  98%"
          }
        },
        "c456742920f748cf829bbd65ff0def11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3c1e93de47c4f8cbe1206dc410eedfe",
            "max": 400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_edb8910f1b4a4343934d6dce5bcd3117",
            "value": 400
          }
        },
        "e07c541eb52b434ab99b9a55c9dc5f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f97ca81966454f9c98dc56940500d745",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_30e51c79ea674b79850ac8ae6c5b8b5c",
            "value": " 394/400 [00:06&lt;00:00, 74.67 examples/s]"
          }
        },
        "28ee590379b24ebb990177700c1d9530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "4c879394f29f4a17908b04da46569516": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94f0d85019484ee490f4f94c3613808f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3c1e93de47c4f8cbe1206dc410eedfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edb8910f1b4a4343934d6dce5bcd3117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f97ca81966454f9c98dc56940500d745": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30e51c79ea674b79850ac8ae6c5b8b5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}