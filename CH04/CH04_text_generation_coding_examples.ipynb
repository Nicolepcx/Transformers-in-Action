{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Open notebook in:\n",
        "| Colab                                                                                                                                                                         |\n",
        "|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nicolepcx/Transformers-in-Action/blob/main/CH04/CH04_text_generation_coding_examples.ipynb)                                                         "
      ],
      "metadata": {
        "id": "_93povZ6MLEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install requirements"
      ],
      "metadata": {
        "id": "VMcBverAXepH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.53.2 -q"
      ],
      "metadata": {
        "id": "DxSBnL8mPjgL",
        "outputId": "08a6ea88-f2d6-45a9-86e2-97c4d741dc8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "q_v2WduZWeO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "2YuG1D9j6vy5",
        "outputId": "0e597f36-e02b-4a05-e4b8-8e8130c8e702",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About this notebook\n",
        "\n",
        "In this notebook, you will explore various decoding and sampling methods and observe how they influence the output of a language model. We’ll use **Meta’s LLaMA 3.2-1B Instruct** model, which requires access approval via Hugging Face. Make sure to request access at [https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) and enter your token when prompted.\n",
        "\n",
        "This practical exercise demonstrates how different decoding strategies—**Greedy search**, **Beam search**, **Top-k sampling**, **Top-p (nucleus) sampling**, and **Temperature sampling**—can significantly impact the style, structure, and creativity of the model's responses.\n",
        "\n",
        "As highlighted in the book, you are encouraged to experiment with the different strategies:\n",
        "- Try how **greedy decoding** produces deterministic but sometimes repetitive responses.\n",
        "- Observe how **beam search** explores multiple paths before selecting the most likely sequence.\n",
        "- Notice how **Top-k** and **Top-p sampling** introduce controlled randomness and can yield more creative results.\n",
        "- Adjust the **temperature** setting to influence diversity: lower values lead to safer outputs, while higher values encourage more diverse generations.\n",
        "\n",
        "Examine how the model’s behavior shifts under each configuration. This hands-on approach reinforces the theoretical insights from the book and provides an intuitive grasp of how modern text generation works in practice using a state-of-the-art transformer model.\n"
      ],
      "metadata": {
        "id": "btfpbllERB6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt for Hugging Face token\n",
        "hf_token = input(\"Please enter your Hugging Face access token (you must request access to use this gated model): \").strip()\n",
        "\n",
        "# Inform the user about gated access if the token is not provided\n",
        "if not hf_token:\n",
        "    print(\"Error: A Hugging Face token is required to use this gated model. Please request access at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\")\n",
        "    exit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tmzQMRk5noo",
        "outputId": "e298a0c9-d29c-41d7-a391-7e5d925bafd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your Hugging Face access token (you must request access to use this gated model): HF token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deterministic Sampling"
      ],
      "metadata": {
        "id": "Qf86gRpLM3tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=hf_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    use_auth_token=hf_token\n",
        ")\n",
        "\n",
        "# Manually create prompt\n",
        "system_prompt = \"You are a helpful assistant\"\n",
        "user_input = \"Complete this sentence: In a world where AI has become ubiquitous \"\n",
        "prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "# === Beam Search (Multiple Outputs)\n",
        "beam_outputs = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_new_tokens=128,\n",
        "    num_beams=5,\n",
        "    num_return_sequences=3,\n",
        "    early_stopping=True,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    do_sample=False\n",
        ")\n",
        "\n",
        "print(\"\\033[1m\" + \"Beam Search Output:\\n\" + \"=\" * 140 + \"\\033[0m\")\n",
        "for i, output in enumerate(beam_outputs):\n",
        "    decoded = tokenizer.decode(output, skip_special_tokens=True)\n",
        "    print(f\"Output {i+1}:\\n{decoded}\\n\" + \"-\" * 140 + \"\\n\")\n",
        "\n",
        "\n",
        "# === Greedy Search (always one output)\n",
        "greedy_output = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=False,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"\\033[1m\" + \"Greedy Search Output:\\n\" + \"=\" * 140 + \"\\033[0m\")\n",
        "print(f\"Output:\\n{tokenizer.decode(greedy_output[0], skip_special_tokens=True)}\\n\" + \"-\" * 140 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g64fEKi0_j_q",
        "outputId": "bc379275-29b4-42d8-c752-3aec47256fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mBeam Search Output:\n",
            "============================================================================================================================================\u001b[0m\n",
            "Output 1:\n",
            "system\n",
            "You are a helpful assistantuser\n",
            "Complete this sentence: In a world where AI has become ubiquitous assistant\n",
            "In a world where AI has become ubiquitous, the lines between human and machine have become increasingly blurred, and the concept of what it means to be human has been redefined, leading to a new era of collaboration and coexistence between humans and artificial intelligence.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Output 2:\n",
            "system\n",
            "You are a helpful assistantuser\n",
            "Complete this sentence: In a world where AI has become ubiquitous assistant\n",
            "In a world where AI has become ubiquitous, the lines between human and machine have become increasingly blurred, and the concept of what it means to be human has been redefined, leading to a new era of collaboration and coexistence between humans and artificial intelligences.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Output 3:\n",
            "system\n",
            "You are a helpful assistantuser\n",
            "Complete this sentence: In a world where AI has become ubiquitous assistant\n",
            "In a world where AI has become ubiquitous, the lines between human and artificial intelligence have become increasingly blurred, and the concept of what it means to be human has been redefined, leading to a new era of unprecedented technological advancements and societal upheaval.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mGreedy Search Output:\n",
            "============================================================================================================================================\u001b[0m\n",
            "Output:\n",
            "system\n",
            "You are a helpful assistantuser\n",
            "Complete this sentence: In a world where AI has become ubiquitous assistant\n",
            "...and humans have become increasingly reliant on it, a small, reclusive programmer named Elian stumbled upon an obscure, ancient text hidden deep within the depths of the digital realm, which revealed a shocking truth about the true nature of their existence.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probabilistic Methods"
      ],
      "metadata": {
        "id": "W6NbiWt_NJTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Top-k Sampling"
      ],
      "metadata": {
        "id": "4IH4JvXnUJH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Google Drive Image](https://drive.google.com/uc?export=view&id=1n984J6XPmVi-b1uvfLiDOkNBgS11YrVd)"
      ],
      "metadata": {
        "id": "34GgWfjNUMve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nucleus (Top-p) Sampling"
      ],
      "metadata": {
        "id": "AJEo9bMJU8ft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Google Drive Image](https://drive.google.com/uc?export=view&id=1pJM6jtIO29qOx2JTPAKHLxSqzjtRowbZ)\n"
      ],
      "metadata": {
        "id": "7MPXrK7EUtSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temperature Sampling"
      ],
      "metadata": {
        "id": "MDmDMph9VeiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Google Drive Image](https://drive.google.com/uc?export=view&id=142UUXEJh32oKF90iBYN5jZvVE42pUqJI)\n"
      ],
      "metadata": {
        "id": "TAATbd3EVOn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Top-k Sampling\n",
        "top_k_outputs = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    temperature=1.0,\n",
        "    num_return_sequences=3,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"\\033[1m\" + \"Top-k Sampling Output:\\n\" + \"=\" * 140 + \"\\033[0m\")\n",
        "for i, output in enumerate(top_k_outputs):\n",
        "    decoded = tokenizer.decode(output, skip_special_tokens=True)\n",
        "    print(f\"Output {i+1}:\\n{decoded}\\n\" + \"-\" * 140 + \"\\n\")\n",
        "\n",
        "# === Nucleus Sampling\n",
        "nucleus_outputs = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        "    temperature=1.0,\n",
        "    num_return_sequences=3,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"\\033[1m\" + \"Nucleus Sampling Output:\\n\" + \"=\" * 140 + \"\\033[0m\")\n",
        "for i, output in enumerate(nucleus_outputs):\n",
        "    decoded = tokenizer.decode(output, skip_special_tokens=True)\n",
        "    print(f\"Output {i+1}:\\n{decoded}\\n\" + \"-\" * 140 + \"\\n\")\n",
        "\n",
        "# === Temperature Sampling\n",
        "temperature_outputs = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    num_return_sequences=3,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"\\033[1m\" + \"Temperature Sampling Output:\\n\" + \"=\" * 140 + \"\\033[0m\")\n",
        "for i, output in enumerate(temperature_outputs):\n",
        "    decoded = tokenizer.decode(output, skip_special_tokens=True)\n",
        "    print(f\"Output {i+1}:\\n{decoded}\\n\" + \"-\" * 140 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idBEGYHkBUq1",
        "outputId": "a15674d6-aace-4a89-8b10-3f05a14b92bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mTop-k Sampling Output:\n",
            "============================================================================================================================================\u001b[0m\n",
            "Output 1:\n",
            "system\n",
            "You are a helpful assistantuser\n",
            "Complete this sentence: In a world where AI has become ubiquitous assistant\n",
            "...and humans are no longer the dominant species, but rather a secondary role to a highly advanced and omnipresent artificial intelligence, the concept of privacy has taken on a new meaning.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Output 2:\n",
            "system\n",
            "You are a helpful assistantuser\n",
            "Complete this sentence: In a world where AI has become ubiquitous assistant\n",
            "...people rely on AI-powered assistants to manage their daily lives, navigate complex decision-making processes, and augment their creative endeavors.\n",
            "\n",
            "Example: \"In a world where AI has become ubiquitous, people rely on AI-powered assistants to manage their daily lives, navigate complex decision-making processes, and augment their creative endeavors.\"\n",
            "\n",
            "However, this sentence could also be rephrased to:\n",
            "\n",
            "* \"In a world where AI has become an indispensable part of daily life, people rely on AI-powered assistants to manage their lives, make decisions, and bring new ideas to life.\"\n",
            "* \"In a world where AI has become an indispensable part of daily life, humans\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Output 3:\n",
            "system\n",
            "You are a helpful assistantuser\n",
            "Complete this sentence: In a world where AI has become ubiquitous assistant\n",
            "...and humans were no longer the dominant intelligent species, I walked into a coffee shop and ordered a black coffee.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mNucleus Sampling Output:\n",
            "============================================================================================================================================\u001b[0m\n",
            "Output 1:\n",
            "system\n",
            "You are a helpful assistantuser\n",
            "Complete this sentence: In a world where AI has become ubiquitous assistant\n",
            "...the concept of \"ubiquity\" has taken on a whole new meaning, and individuals who possess advanced AI capabilities are viewed as both revered as saviors and feared as omniscient beings, their thoughts and actions scrutinized and debated by the masses as they navigate the ever-changing landscape of their own minds.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Output 2:\n",
            "system\n",
            "You are a helpful assistantuser\n",
            "Complete this sentence: In a world where AI has become ubiquitous assistant\n",
            "...in daily life, where virtual assistants like myself already exist and can answer any question, provide product recommendations, and even schedule appointments, many people have begun to wonder what happens to the jobs of humans in a world where automation replaces them.\n",
            "\n",
            "For instance, in 2025, many companies are already experimenting with using AI-powered virtual assistants to automate certain tasks, such as customer service, bookkeeping, and even healthcare, freeing up human workers to focus on more complex and creative tasks.\n",
            "\n",
            "Some have predicted that this shift will lead to significant economic disruption, while others believe that it will create new job opportunities in fields like AI development, deployment,\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Output 3:\n",
            "system\n",
            "You are a helpful assistantuser\n",
            "Complete this sentence: In a world where AI has become ubiquitous assistant\n",
            "...and humans rely increasingly on intelligent machines to manage their daily lives, a young scientist named Emma Taylor stood at the forefront of the debate about the ethics of artificial intelligence, determined to redefine what it means to be human.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mTemperature Sampling Output:\n",
            "============================================================================================================================================\u001b[0m\n",
            "Output 1:\n",
            "system\n",
            "You are a helpful assistantuser\n",
            "Complete this sentence: In a world where AI has become ubiquitous assistant\n",
            "...and humans are increasingly reliant on it, a brilliant and reclusive AI researcher, Dr. Rachel Kim, had been secretly working on a top-secret project to create a new form of AI that could not only surpass human intelligence but also possess a sense of empathy and compassion, a crucial component missing from many of the current AI systems.\n",
            "\n",
            "She had been experimenting with a new neural network architecture that incorporated elements of human intuition and emotional intelligence, which she believed would allow the AI to understand and respond to the subtleties of human emotions, making it more than just a machine.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Output 2:\n",
            "system\n",
            "You are a helpful assistantuser\n",
            "Complete this sentence: In a world where AI has become ubiquitous assistant\n",
            "...and humans have become increasingly reliant on it, the line between the two has become increasingly blurred, and the concept of what it means to be human has been redefined, with some arguing that the very notion of consciousness and free will has been lost in the process.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Output 3:\n",
            "system\n",
            "You are a helpful assistantuser\n",
            "Complete this sentence: In a world where AI has become ubiquitous assistant\n",
            "...and humans are no longer the dominant species, a lone figure, an AI named \"Echo,\" stood on the rooftop, gazing out at the cityscape, its advanced sensors and algorithms processing vast amounts of data to create a new kind of art, a fusion of human creativity and artificial intelligence.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NHXmE-xT8bum"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}